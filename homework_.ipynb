{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.special import softmax \n",
    "from scipy.stats import bernoulli\n",
    "import time \n",
    "from scipy.optimize import minimize\n",
    "from ddm import pdf # ddm probility density function\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from utils.env import frozen_lake\n",
    "from utils.viz import viz \n",
    "viz.get_style()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业主要涉及两种参数估计方法(MLE+MCMC)和强化学习(dynamic programming+Q-learning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 参数估计之MLE\n",
    "* 采用`exampledata.txt`的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.loadtxt('exampledata.txt')\n",
    "print(data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "**Q1.1: 请写出ddm的负对数似然函数`negloglikeli`(negative log-likelihood function)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a negative log-likelihood objective functions\n",
    "def negloglikeli(params):\n",
    "    '''\n",
    "    <params>:(4,) array, drift coefficient, decision boundary, initial bias, non-decision time\n",
    "    '''\n",
    "    # specify your loaded params\n",
    "    ##------------------------------##\n",
    "    ##           your answer        ##\n",
    "    ##------------------------------##\n",
    "    # do initialization    \n",
    "    ##------------------------------##\n",
    "    ##           your answer        ##\n",
    "    ##------------------------------##\n",
    "    # loop trial\n",
    "    ##------------------------------##\n",
    "    ##           your answer        ##\n",
    "    ##------------------------------##\n",
    "    pp=0.999*pp + np.finfo(np.float32).eps # to avoid p=0\n",
    "    # take log, sum，add negative\n",
    "    return ##      your answer      ##\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "**Q1.2: 利用最大似然估计(maximum likelihood estimation)的方法求解ddm模型参数k,b.a,ndt**\n",
    "\n",
    "参数k,b.a,ndt的bounds分别为((0, 20), (0, 5), (0, 1), (0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now start your MLE\n",
    "\n",
    "# print your result\n",
    "print('\\nfitted drift coefficient is ', res.x[0])\n",
    "print('fitted decision boundary is ', res.x[1])\n",
    "print('fitted initial bias is ', res.x[2])\n",
    "print('fitted nondecision time is ', res.x[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 参数估计之MCMC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "**Q2: 利用Metropolitan-Hasting的方法求解DDM模型参数.不能借助工具包，必须手写M-H**\n",
    "\n",
    "hint：\n",
    "* 提议分布建议使用正态分布\n",
    "* 注意每个输入参数的取值范围(especially:ndt)\n",
    "* $max[\\frac{f(x)}{f(y)}, 1] = max[log(f(x))-log(f(y)), 0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, uniform\n",
    "\n",
    "def metropolis_hastings(data, n_sample=10000, n_burnin=1500):\n",
    "    sample_chain = []\n",
    "    # 参数的初始化\n",
    "    ##---------------------------------------##\n",
    "    ##               your answer             ##\n",
    "    ##---------------------------------------##\n",
    "    for i in range(n_sample + n_burnin):\n",
    "        ##---------------------------------------##\n",
    "        ##               your answer             ##\n",
    "        ##---------------------------------------##  \n",
    "           \n",
    "        # burn-in 之后才开始收集参数\n",
    "        if i >= n_burnin:\n",
    "            sample_chain.append(current_param)\n",
    "\n",
    "    return np.array(sample_chain)\n",
    "\n",
    "\n",
    "# 开始M-H采样\n",
    "samples = metropolis_hastings(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 强化学习之dynamic programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "我们接下来的游戏需要借助一个冰湖任务完成。\n",
    "\n",
    "这个游戏的规则很简单：掉进蓝色冰窟窿内即失败(游戏结束)；顺利到达goal(G)即成功。\n",
    "\n",
    "先来看看我们已经拥有的冰湖任务的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "env = frozen_lake(seed=seed)\n",
    "env.reset()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "env.render(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个冰湖环境中，agent的动作为：\n",
    "* 0: 上\n",
    "* 1: 下\n",
    "* 2: 左\n",
    "* 3: 右\n",
    "\n",
    "这个冰湖的形状是8*8的。每一个小方块就是一个state，即，state形状为8 * 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "**Q3.1：现在有nA个action，nS个state。请先随机生成一个policy。每一行代表一个state，每一列代表一个动作.并且令inverse temperature = 5**\n",
    "\n",
    "* inverse temperature(beta)→softmax中加入 beta：$ \\frac{e^{\\beta*z_i}}{Σ_i e^{\\beta*z_I}} $\n",
    "\n",
    "* 你可以直接调用softmax这个函数，且加入inv temp的形式为：$p = softmax(A*\\beta,axis=1)$, A为矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a random policy\n",
    "seed = 1234\n",
    "rng = np.random.RandomState(seed)\n",
    "###-------------###\n",
    "### your answer ###\n",
    "###-------------###\n",
    "print(pi_rand)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "**Q3.2：定义一个评估policy的函数。你需要： 1.在已知当前步的state、action和transition function的情况下，估计下一个可能state的reward和value；2.根据v_new 和 v_old，判断收敛**\n",
    "\n",
    "hint：\n",
    "\n",
    "* reward和value的计算可以参考env.r()\n",
    "\n",
    "* delta的计算公式是：$ delta = max(delta,|v_{old}-v_{new}|)$\n",
    "\n",
    "* check convergence就是为了令：\n",
    "\n",
    "* $delta<theta(theta为一个极小值)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_eval(pi, V, env, theta=1e-4, gamma=.99):\n",
    "    for s in env.s_termination:\n",
    "        V[s] = 0 \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.S:\n",
    "            if s not in env.s_termination:\n",
    "                v_old = V[s].copy()\n",
    "                v_new = 0\n",
    "                for a in env.A:\n",
    "                    p = env.p_s_next(s,a)\n",
    "                    for s_next in env.S:\n",
    "                    ###-------------------------------###\n",
    "                    ###         your answer           ###\n",
    "                    ###-------------------------------###\n",
    "              V[s] = v_new\n",
    "                ## check convergence\n",
    "                    ###-------------------------------###\n",
    "                    ###         your answer           ###\n",
    "                    ###-------------------------------###            \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "        clear_output(True)\n",
    "        env.show_v(ax, V)\n",
    "        time.sleep(1)\n",
    "        plt.show()   \n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可视化\n",
    "V = np.zeros([env.nS])\n",
    "V = policy_eval(pi_rand, V, env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  随机生成的policy显然是不够好的，现在我们就来试着改进policy\n",
    "**Q3.3：改进policy。你需要： 1.在已知当前步的state、action和transition function的情况下，估计下一个可能state的reward和value；2.计算pi[s]，并且循环直至平稳**\n",
    "\n",
    "hint：\n",
    "\n",
    "* $q += Σ_a P(s'|s,a)*[r_t+gamma*V(s')]$\n",
    "    \n",
    "* 在对应的state下，pi[s]就是意味着能返回q最大值的action\n",
    "\n",
    "* 如果新的pi和原先的pi之间的差值$ delta<theta(极小值) $，我们认为这个policy稳定。也就是stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## policy_improv\n",
    "def policy_improv(pi, V, env, theta=1e-4, gamma=.99):\n",
    "    pi_old = pi.copy()\n",
    "    for s in env.S:\n",
    "        # 每一次循环都要重新初始化包含所有action的 q\n",
    "        q = np.zeros([env.nA])\n",
    "        for a in env.A:\n",
    "            p = env.p_s_next(s,a)\n",
    "            for s_next in env.S:\n",
    "                ###-------------------------------###\n",
    "                ###         your answer           ###\n",
    "                ###-------------------------------###\n",
    "    # loop until stable\n",
    "    ###-------------------------------###\n",
    "    ###         your answer           ###\n",
    "    ###-------------------------------###\n",
    "    return pi, stable   \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "**Q3.4：完成policy iteration。你需要：1.首先评估policy得到V(value),2.根据得到的V，pi，env来完成迭代**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iter(env, seed=1234):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    V = rng.randn(env.nS)*0.0001\n",
    "    # initialize V 除goal为0\n",
    "    for s in env.s_termination:\n",
    "        V[s] = 0\n",
    "        ## 先随意生成一个pi(policy)\n",
    "        ###-------------------------------###\n",
    "        ###         your answer           ###\n",
    "        ###-------------------------------###\n",
    "\n",
    "    while True:\n",
    "        ###-------------------------------###\n",
    "        ###         your answer           ###\n",
    "        ###-------------------------------###\n",
    "        # visualize \n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        clear_output(True)\n",
    "        ax = axs[0]\n",
    "        env.show_v(ax, V)\n",
    "        ax = axs[1]\n",
    "        env.show_pi(ax, pi)\n",
    "        time.sleep(.1)\n",
    "        plt.show()   \n",
    "        \n",
    "        if stable:break\n",
    "    return V, pi  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 我们来看一下policy iteration的结果\n",
    "V1, pi1 = policy_iter(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "**Q3.5：完成value iteration。提示步骤已在代码中**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(env, seed=1234, theta=1e-4, gamma=.99):\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    # initialize V(s), arbitrarily except V(terminal)=0\n",
    "    V = np.zeros(env.nS)\n",
    "    if s in env.s_termination:\n",
    "        V[s] = 0\n",
    "    # init policy \n",
    "    pi = softmax(rng.randn(env.nS,env.nA)*5, axis=1)    \n",
    "    # loop until converge\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.S:\n",
    "            v_old = V[s].copy()\n",
    "            v_new = 0\n",
    "            for a in env.A:\n",
    "                p = env.p_s_next(s, a)\n",
    "                for s_next in env.S:\n",
    "                    # calculate v_new\n",
    "                    ###-------------------------------###\n",
    "                    ###         your answer           ###\n",
    "                    ###-------------------------------###\n",
    "            # calculate V\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###\n",
    "            # get new policy \n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###\n",
    "            # calculate delta\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###\n",
    "        if delta < theta:\n",
    "            break \n",
    "    for s in env.s_termination:\n",
    "        V[s] = 0\n",
    "    return V, pi "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 强化学习中之 Q-learning (sarsa)\n",
    "\"transition function and reward fucntion cannot be always known\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "**Q4.1：写一段函数，随机生成action**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(q, rng, env, eps):\n",
    "    a_max = np.argwhere(q==np.max(q)).flatten()\n",
    "    policy = np.sum([np.eye(env.nA)[i] for i in a_max], axis=0) / len(a_max)\n",
    "    if rng.rand() < 1-eps:\n",
    "        # you know the policy,set ps according to your pi\n",
    "        ## -----------------------##\n",
    "        ##      your answer       ##\n",
    "        ## -----------------------##   \n",
    "    else:\n",
    "        # just random\n",
    "        a = rng.choice(env.nA)\n",
    "    return a "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "**Q4.2：补充sarsa 代码**\n",
    "\n",
    "hint：\n",
    "* Sarsa建立在TD-learning 基础上，它的核心是：\n",
    "\n",
    "    $Q_{(s,a)} = Q_{(s,a)} + \\alpha * [r + \\gamma*Q_{(s',a')}-Q_{(s,a)}]$\n",
    "* 可以借助env.step()这个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, alpha=.1, eps=.1, gamma=.99, max_epi=2000, seed=1234, theta=1e-4):\n",
    "    # rng\n",
    "    rng = np.random.RandomState(seed)\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    for epi in range(max_epi):\n",
    "        s, r, done = env.reset()\n",
    "        q_old = Q.copy()\n",
    "        G = 0\n",
    "        while True:\n",
    "            # sample At, observe Rt, St+1\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------### \n",
    "            # calc s_next\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------### \n",
    "            # given s_next, calc a_next\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###   \n",
    "            # update Q,s,a\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###       \n",
    "            G += r                          \n",
    "            if done:\n",
    "                break     \n",
    "        if (np.abs(q_old - Q)<theta).all():\n",
    "            break\n",
    "    pi = np.eye(env.nA)[np.argmax(Q, axis=1)]\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可视化\n",
    "Q_sarsa, pi_sarsa = Sarsa(env)\n",
    "V3 = Q_sarsa.max(1)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax = axs[0]\n",
    "env.show_v(ax, V3)\n",
    "ax = axs[1]\n",
    "env.show_pi(ax, pi_sarsa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
