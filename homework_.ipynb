{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.special import softmax \n",
    "from scipy.stats import bernoulli\n",
    "import time \n",
    "from scipy.optimize import minimize\n",
    "from ddm import pdf # ddm probility density function\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from utils.env import frozen_lake\n",
    "from utils.viz import viz \n",
    "viz.get_style()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业主要涉及两种参数估计方法(MLE+MCMC)和强化学习(dynamic programming+Sarsa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 参数估计之MLE\n",
    "* 采用`exampledata.txt`的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data\n",
    "data = np.loadtxt('example.txt')\n",
    "print(data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "**Q1.1: 请写出ddm的负对数似然函数`negloglikeli`(negative log-likelihood function)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a negative log-likelihood objective functions\n",
    "def negloglikeli(params):\n",
    "    '''\n",
    "    <params>:(4,) array, drift coefficient, decision boundary, initial bias, non-decision time\n",
    "    '''\n",
    "    # specify your loaded params\n",
    "    ##------------------------------##\n",
    "    ##           your answer        ##\n",
    "    ##------------------------------##\n",
    "    # do initialization    \n",
    "    ##------------------------------##\n",
    "    ##           your answer        ##\n",
    "    ##------------------------------##\n",
    "    # loop trial\n",
    "    ##------------------------------##\n",
    "    ##           your answer        ##\n",
    "    ##------------------------------##\n",
    "    pp=0.999*pp + np.finfo(np.float32).eps # to avoid p=0\n",
    "    # take log, sum，add negative\n",
    "    return ##      your answer      ##\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "**Q1.2: 利用最大似然估计(maximum likelihood estimation)的方法求解ddm模型参数k,b.a,ndt**\n",
    "* 参数k,b.a,ndt的bounds分别为((0, 20), (0, 5), (0, 1), (0, 1))\n",
    "* 请使用example.txt。其中1-3列分别是：coherence(运动序列。为了简化，只有一种coherence)，response time(总反应时长)，correct(作答是否正确)\n",
    "* example.txt的形状为(3000,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "## now start your MLE\n",
    "\n",
    "# print your result\n",
    "print('\\nfitted drift coefficient is ', res.x[0])\n",
    "print('fitted decision boundary is ', res.x[1])\n",
    "print('fitted initial bias is ', res.x[2])\n",
    "print('fitted nondecision time is ', res.x[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正确答案如下：\n",
    "\n",
    "fitted drift coefficient is  1.0311682099419661\n",
    "\n",
    "fitted decision boundary is  2.3827290342577285\n",
    "\n",
    "fitted initial bias is  0.4972358161521576\n",
    "\n",
    "fitted nondecision time is  0.20728465380894037"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 参数估计之MCMC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "**Q2: 利用Metropolitan-Hasting的方法估计DDM模型中的参数：drift rate .不能借助工具包，必须手写M-H**\n",
    "* 请使用example.txt。\n",
    "* 将DDM中其余参数固定为： \n",
    "    - B(boundary) = 2\n",
    "    - a(initial bias) = 0.5\n",
    "    - ndt(non-decision time) = 0.1\n",
    "* (代码中已经固定randomseed，每一次结果都将一致。请勿修改)\n",
    "* hint：\n",
    "    - 提议分布建议使用正态分布。正态分布可以使用scipy.stats.norm()\n",
    "    - 从提议分布中每一次采样，起始值都设置为每一次循环初始的initial值\n",
    "    - 从提议分布中采样的结果通过exp()转换（因为drift rate始终大于零）\n",
    "    - $min[\\frac{f(x)}{f(y)}, 1] = min[log(f(x))-log(f(y)), 0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, uniform\n",
    "\n",
    "def metropolis_hastings(data, y_k, n_sample=30000, n_burnin=10000, random_seed=1234):\n",
    "    np.random.seed(random_seed)\n",
    "    ##---------------------------------------##\n",
    "    ##               your answer             ##\n",
    "    ##---------------------------------------##\n",
    "    for i in range(n_sample + n_burnin):\n",
    "        ##---------------------------------------##\n",
    "        ##               your answer             ##\n",
    "        ##---------------------------------------##  \n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "# 开始M-H采样\n",
    "samples = metropolis_hastings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "plt.hist(samples, bins=15, density=True, label='samples')\n",
    "plt.xlabel('drift rate')\n",
    "plt.ylabel('Density')\n",
    "plt.title(' Estimation of Samples')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正确答案如下图"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='MH.png' width='400' height='300'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 强化学习之dynamic programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "我们接下来的游戏需要借助一个冰湖任务完成。这个游戏的规则很简单：掉进蓝色冰窟窿内即失败(游戏结束)；顺利到达goal(G)即成功。\n",
    "在这个冰湖环境中，agent的动作为：\n",
    "* 0: 上\n",
    "* 1: 下\n",
    "* 2: 左\n",
    "* 3: 右\n",
    "##### 先来看看我们已经拥有的冰湖任务的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "env = frozen_lake(seed=seed)\n",
    "env.reset()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "env.render(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "**Q3：完成value iteration。**\n",
    "完成value iteration的步骤(伪代码)：\n",
    "1. Initialize V(s) arbitrarily for all s ∈ States(except termination)\n",
    "2. loop \n",
    "    1. $ Δ ← 0$\n",
    "    2. loop for each state s ∈ States\n",
    "        1. $ v ← V(s)$\n",
    "        2. $ V(s) ← max_a Σ_s' p(s'|s, a) * [R(s') + \\gamma * V(s')]$\n",
    "        3. $Δ ← max(Δ, |v - V(s)|)$\n",
    "3. loop until $Δ < \\theta$\n",
    "4. $ pi(s) = argmax_a Σ_{s'} p(s'|s, a) * [R(s') + \\gamma * V(s')]$\n",
    "5. return $V,pi$\n",
    "    \n",
    "hint：\n",
    "* 可以使用env.r()获取下一个状态的reward\n",
    "* 可以使用env.p_s_next()来获取状态转移概率\n",
    "* 可以借助np.eye()来找到能返回最大值的action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(env, seed=1234, theta=1e-4, gamma=.99):\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    # initialize V(s), arbitrarily except V(terminal)=0\n",
    "    V = np.zeros(env.nS)\n",
    "    if s in env.s_termination:\n",
    "        V[s] = 0\n",
    "    # init policy \n",
    "    pi = softmax(rng.randn(env.nS,env.nA)*5, axis=1)    \n",
    "    # loop until converge\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.S:\n",
    "            v_old = V[s].copy()\n",
    "            v_new = 0\n",
    "            for a in env.A:\n",
    "                p = env.p_s_next(s, a)\n",
    "                for s_next in env.S:\n",
    "                    # calculate v_new\n",
    "                    ###-------------------------------###\n",
    "                    ###         your answer           ###\n",
    "                    ###-------------------------------###\n",
    "            # calculate V\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###\n",
    "            # get new policy \n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###\n",
    "            # calculate delta\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###\n",
    "        \n",
    "        # visualize \n",
    "        if show_update:\n",
    "            _, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "            clear_output(True)\n",
    "            ax = axs[0]\n",
    "            env.show_v(ax, V)\n",
    "            ax = axs[1]\n",
    "            env.show_pi(ax, pi)\n",
    "            time.sleep(.1)\n",
    "            plt.show()\n",
    "        # check covergence \n",
    "        if delta < theta:\n",
    "            break \n",
    "    for s in env.s_termination:\n",
    "        V[s] = 0\n",
    "    return V, pi "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 强化学习中之 sarsa (TD-learning)\n",
    "上节课我们讲的内容是Q-learning。SARSA和Q-learning都属于强化学习中的TD-learning范畴，因为它们都使用了时序差分学习(TD-learning)的思想来更新值函数。虽然它们的目标相同，但它们在更新策略上有一些不同之处。\n",
    "\n",
    "1. **目标**：\n",
    "   - SARSA的目标是学习最优的动作值函数Q，并根据该值函数选择动作。它基于当前状态-动作对的值更新。\n",
    "   - Q-learning的目标是学习最优的状态值函数Q，并根据该值函数选择动作。它基于当前状态下所有可能动作的最大值更新。\n",
    "\n",
    "2. **更新策略**：\n",
    "   - 在SARSA中，Q值的更新考虑了当前状态下采取的动作以及在下一个状态下采取的动作。具体地说，更新公式为：\n",
    "            $Q_{(s,a)} = Q_{(s,a)} + \\alpha * [r + \\gamma*Q_{(s',a')}-Q_{(s,a)}]$\n",
    "   - 其中，\\(s\\)是当前状态，\\(a\\)是当前动作，\\(r\\)是获得的奖励，\\(s'\\)是下一个状态，\\(a'\\)是下一个状态下采取的动作，\\(α\\)是   学习率，\\(γ\\)是折扣因子。\n",
    "   - 在Q-learning中，Q值的更新只考虑了当前状态下所有可能的动作中的最大值。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正确答案如下图"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='value_and_policy.png' width='800' height='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(q, rng, env, eps):\n",
    "    a_max = np.argwhere(q==np.max(q)).flatten()\n",
    "    policy = np.sum([np.eye(env.nA)[i] for i in a_max], axis=0) / len(a_max)\n",
    "    if rng.rand() < 1-eps:\n",
    "        a = rng.choice(env.nA, p=policy)\n",
    "    else:\n",
    "        a = rng.choice(env.nA)\n",
    "    return a "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "**Q4：补充sarsa 代码**\n",
    "\n",
    "我们来看一下实现Sarsa的步骤：\n",
    "1. for episode = 1 to episodes :\n",
    "    1. Initialize state s\n",
    "    2. Choose action a using ε-greedy policy based on Q(s, a)\n",
    "    3. loop\n",
    "        1. Take action a, observe reward r and next state s'\n",
    "        2. Choose action a' using ε-greedy policy based on Q(s', a')\n",
    "        3. Q(s, a) ← Q(s, a) + α * [r + γ * Q(s', a') - Q(s, a)] \n",
    "        4. s ← s'\n",
    "        5. a ← a'\n",
    "    4. loop until s is terminal\n",
    "\n",
    "2. episode loop 结束，return Q\n",
    "\n",
    "hint\n",
    "* episode是指训练的总次数\n",
    "*  这一步：Q(s, a) ← Q(s, a) + α * [r + γ * Q(s', a') - Q(s, a)] ，需要考虑下一个状态是否为done(终点)。\n",
    "    - 如果是：Q(s, a) ← Q(s, a) + α * [r + γ * 0 - Q(s, a)]\n",
    "    - else：Q(s, a) ← Q(s, a) + α * [r + γ * Q(s', a') - Q(s, a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, alpha=.1, eps=.1, gamma=.99, max_epi=2000, seed=1234, theta=1e-4):\n",
    "    # rng\n",
    "    rng = np.random.RandomState(seed)\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    for epi in range(max_epi):\n",
    "        s, r, done = env.reset()\n",
    "        q_old = Q.copy()\n",
    "        G = 0\n",
    "        while True:\n",
    "            # sample At, observe Rt, St+1\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------### \n",
    "            # calc s_next\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------### \n",
    "            # given s_next, calc a_next\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###   \n",
    "            # update Q,s,a\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###       \n",
    "            G += r                          \n",
    "            if done:\n",
    "                break     \n",
    "        if (np.abs(q_old - Q)<theta).all():\n",
    "            break\n",
    "    pi = np.eye(env.nA)[np.argmax(Q, axis=1)]\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可视化\n",
    "Q_sarsa, pi_sarsa = Sarsa(env)\n",
    "V3 = Q_sarsa.max(1)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax = axs[0]\n",
    "env.show_v(ax, V3)\n",
    "ax = axs[1]\n",
    "env.show_pi(ax, pi_sarsa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正确答案如下图"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sarsa.png\" width='800' height='400'>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
