{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.special import softmax \n",
    "from scipy.stats import bernoulli\n",
    "import time \n",
    "from scipy.optimize import minimize\n",
    "from ddm import pdf # ddm probility density function\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from utils.env import frozen_lake\n",
    "from utils.viz import viz \n",
    "viz.get_style()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业主要涉及两种参数估计方法(MLE+MCMC)和强化学习(dynamic programming+Q-learning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 参数估计之MLE\n",
    "* 采用`exampledata.txt`的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data\n",
    "data = np.loadtxt('exampledata.txt')\n",
    "print(data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "**Q1.1: 请写出ddm的负对数似然函数`negloglikeli`(negative log-likelihood function)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a negative log-likelihood objective functions\n",
    "def negloglikeli(params):\n",
    "    '''\n",
    "    <params>:(4,) array, drift coefficient, decision boundary, initial bias, non-decision time\n",
    "    '''\n",
    "    # specify your loaded params\n",
    "    ##------------------------------##\n",
    "    ##           your answer        ##\n",
    "    ##------------------------------##\n",
    "    # do initialization    \n",
    "    ##------------------------------##\n",
    "    ##           your answer        ##\n",
    "    ##------------------------------##\n",
    "    # loop trial\n",
    "    ##------------------------------##\n",
    "    ##           your answer        ##\n",
    "    ##------------------------------##\n",
    "    pp=0.999*pp + np.finfo(np.float32).eps # to avoid p=0\n",
    "    # take log, sum，add negative\n",
    "    return ##      your answer      ##\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "**Q1.2: 利用最大似然估计(maximum likelihood estimation)的方法求解ddm模型参数k,b.a,ndt**\n",
    "\n",
    "参数k,b.a,ndt的bounds分别为((0, 20), (0, 5), (0, 1), (0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now start your MLE\n",
    "\n",
    "# print your result\n",
    "print('\\nfitted drift coefficient is ', res.x[0])\n",
    "print('fitted decision boundary is ', res.x[1])\n",
    "print('fitted initial bias is ', res.x[2])\n",
    "print('fitted nondecision time is ', res.x[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 参数估计之MCMC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "**Q2: 利用Metropolitan-Hasting的方法求解DDM模型参数.不能借助工具包，必须手写M-H**\n",
    "\n",
    "hint：\n",
    "* 提议分布建议使用正态分布\n",
    "* 注意每个输入参数的取值范围(especially:ndt)\n",
    "* $max[\\frac{f(x)}{f(y)}, 1] = max[log(f(x))-log(f(y)), 0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, uniform\n",
    "\n",
    "def metropolis_hastings(data, n_sample=10000, n_burnin=1500):\n",
    "    sample_chain = []\n",
    "    # 参数的初始化\n",
    "    ##---------------------------------------##\n",
    "    ##               your answer             ##\n",
    "    ##---------------------------------------##\n",
    "    for i in range(n_sample + n_burnin):\n",
    "        ##---------------------------------------##\n",
    "        ##               your answer             ##\n",
    "        ##---------------------------------------##  \n",
    "           \n",
    "        # burn-in 之后才开始收集参数\n",
    "        if i >= n_burnin:\n",
    "            sample_chain.append(current_param)\n",
    "\n",
    "    return np.array(sample_chain)\n",
    "\n",
    "\n",
    "# 开始M-H采样\n",
    "samples = metropolis_hastings(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 强化学习之dynamic programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "我们接下来的游戏需要借助一个冰湖任务完成。这个游戏的规则很简单：掉进蓝色冰窟窿内即失败(游戏结束)；顺利到达goal(G)即成功。\n",
    "在这个冰湖环境中，agent的动作为：\n",
    "* 0: 上\n",
    "* 1: 下\n",
    "* 2: 左\n",
    "* 3: 右\n",
    "\n",
    "这个冰湖的形状是8*8的。每一个小方块就是一个state，即，state形状为8 * 8\n",
    "##### 先来看看我们已经拥有的冰湖任务的环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "env = frozen_lake(seed=seed)\n",
    "env.reset()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "env.render(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "**Q3：完成value iteration。提示步骤已在代码中**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(env, seed=1234, theta=1e-4, gamma=.99):\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    # initialize V(s), arbitrarily except V(terminal)=0\n",
    "    V = np.zeros(env.nS)\n",
    "    if s in env.s_termination:\n",
    "        V[s] = 0\n",
    "    # init policy \n",
    "    pi = softmax(rng.randn(env.nS,env.nA)*5, axis=1)    \n",
    "    # loop until converge\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.S:\n",
    "            v_old = V[s].copy()\n",
    "            v_new = 0\n",
    "            for a in env.A:\n",
    "                p = env.p_s_next(s, a)\n",
    "                for s_next in env.S:\n",
    "                    # calculate v_new\n",
    "                    ###-------------------------------###\n",
    "                    ###         your answer           ###\n",
    "                    ###-------------------------------###\n",
    "            # calculate V\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###\n",
    "            # get new policy \n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###\n",
    "            # calculate delta\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###\n",
    "        if delta < theta:\n",
    "            break \n",
    "    for s in env.s_termination:\n",
    "        V[s] = 0\n",
    "    return V, pi "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 强化学习中之 TD-learning (sarsa)\n",
    "\"transition function and reward fucntion cannot be always known\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(q, rng, env, eps):\n",
    "    a_max = np.argwhere(q==np.max(q)).flatten()\n",
    "    policy = np.sum([np.eye(env.nA)[i] for i in a_max], axis=0) / len(a_max)\n",
    "    if rng.rand() < 1-eps:\n",
    "        a = rng.choice(env.nA, p=policy)\n",
    "    else:\n",
    "        a = rng.choice(env.nA)\n",
    "    return a "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "**Q4：补充sarsa 代码**\n",
    "\n",
    "hint：\n",
    "* Sarsa建立在TD-learning 基础上，它的核心是：\n",
    "\n",
    "    $Q_{(s,a)} = Q_{(s,a)} + \\alpha * [r + \\gamma*Q_{(s',a')}-Q_{(s,a)}]$\n",
    "* 可以借助env.step()这个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, alpha=.1, eps=.1, gamma=.99, max_epi=2000, seed=1234, theta=1e-4):\n",
    "    # rng\n",
    "    rng = np.random.RandomState(seed)\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    for epi in range(max_epi):\n",
    "        s, r, done = env.reset()\n",
    "        q_old = Q.copy()\n",
    "        G = 0\n",
    "        while True:\n",
    "            # sample At, observe Rt, St+1\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------### \n",
    "            # calc s_next\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------### \n",
    "            # given s_next, calc a_next\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###   \n",
    "            # update Q,s,a\n",
    "            ###-------------------------------###\n",
    "            ###         your answer           ###\n",
    "            ###-------------------------------###       \n",
    "            G += r                          \n",
    "            if done:\n",
    "                break     \n",
    "        if (np.abs(q_old - Q)<theta).all():\n",
    "            break\n",
    "    pi = np.eye(env.nA)[np.argmax(Q, axis=1)]\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可视化\n",
    "Q_sarsa, pi_sarsa = Sarsa(env)\n",
    "V3 = Q_sarsa.max(1)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax = axs[0]\n",
    "env.show_v(ax, V3)\n",
    "ax = axs[1]\n",
    "env.show_pi(ax, pi_sarsa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
